{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HES Tables - scripts for initial merging/chunking by patient ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script I prepare some functions for:\n",
    "\n",
    "* separating positive and negative cohort, for each table\n",
    "\n",
    "* merging the initial tables by year, for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just setting paths to test functions\n",
    "data_path=\"./sample_HES/\"\n",
    "file1= \"HES_Sample_APC.txt\"\n",
    "filename=data_path+file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#creating mock list of positive patient IDs\n",
    "df=pd.read_csv(filename, sep=\"\\t\", low_memory=False)\n",
    "sample_rows = random.sample(list(df.ENCRYPTED_HESID), 50)\n",
    "mock_pos = df[\"ENCRYPTED_HESID\"].ix[sample_rows]\n",
    "mock_pos.to_csv(\"./sample_HES/HES_APC_PosIDs.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that creates appropriate names for new files, maintaing the same extension\n",
    "#it tries to take in account that the filename might be a full path with initial ./\n",
    "def outfile_name(filename, suffix):\n",
    "    name=filename.split(\".\")\n",
    "    if len(name)==2 :\n",
    "        nameout=[name[0]+suffix, name[1]]   \n",
    "    else:\n",
    "        nameout=[\"\",name[1]+suffix, name[2]]\n",
    "    return \".\".join(nameout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and Negative cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that extract positive cohort (for each table)\n",
    "def create_positive_cohort(filename, IDcol_label, pos_IDs, sep=\"\\t\", suffix_out=\"_pos\"):\n",
    "    \"\"\"\n",
    "    --Parameters--\n",
    "    filename: str \n",
    "              File path of the table\n",
    "    IDcol_label: str \n",
    "                 Name of the column that contains the patient IDS (different for each type of dataset)\n",
    "    pos_IDs: str \n",
    "             File path of the positive IDs file\n",
    "    sep: str \n",
    "         File separator for read_csv\n",
    "    suffix_out: str\n",
    "         Suffix that defines the positive cohort files\n",
    "    --Returns--\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename, sep=sep, low_memory=False)\n",
    "    pos_IDs=pd.read_csv(pos_IDs, sep=\"\\t\", low_memory=False)\n",
    "    pos_df=df[df[IDcol_label].isin(pos_IDs)]\n",
    "    fileout=outfile_name(filename, suffix_out)\n",
    "    pos_df.to_csv(fileout,sep=sep,index=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sample_HES/HES_Sample_APC_pos.txt'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile_name(filename, suffix=\"_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample_HES/HES_Sample_APC.txt\n"
     ]
    }
   ],
   "source": [
    "create_positive_cohort (filename, \"ENCRYPTED_HESID\", \"./sample_HES/HES_APC_PosIDs.txt\", sep=\"\\t\", suffix_out=\"_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that filter out positive cohort (for each table)\n",
    "def create_negative_cohort (filename, IDcol_label, pos_IDs, sep=\"\\t\", suffix_out=\"_neg\"):\n",
    "    \"\"\"\n",
    "    --Parameters--\n",
    "    filename: str \n",
    "              File path of the table\n",
    "    IDcol_label: str \n",
    "                 Name of the column that contains the patient IDS (different for each type of dataset)\n",
    "    pos_IDs: str \n",
    "             File path of the positive IDs file\n",
    "    sep: str \n",
    "         File separator for read_csv\n",
    "    suffix_out: str\n",
    "         Suffix that defines the negative cohort files\n",
    "    --Returns--\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename, sep=sep, low_memory=False)\n",
    "    pos_IDs=pd.read_csv(pos_IDs, sep=\"\\t\", low_memory=False)\n",
    "    neg_df=df[~df[IDcol_label].isin(pos_IDs)]\n",
    "    fileout=outfile_name(filename, suffix_out)\n",
    "    neg_df.to_csv(fileout,sep=sep,index=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_negative_cohort (filename, \"ENCRYPTED_HESID\", \"./sample_HES/HES_APC_PosIDs.txt\", sep=\"\\t\", suffix_out=\"_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 475)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test: inner join of positive and negative cohorts must be empty\n",
    "pos=pd.read_csv(\"./sample_HES/HES_Sample_APC_pos.txt\", sep=\"\\t\", low_memory=False)\n",
    "neg=pd.read_csv(\"./sample_HES/HES_Sample_APC_neg.txt\", sep=\"\\t\", low_memory=False)\n",
    "\n",
    "merged_inner = neg.merge(pos, left_on='ENCRYPTED_HESID', right_on='ENCRYPTED_HESID', how='inner')\n",
    "\n",
    "# what's the size of the output data?\n",
    "merged_inner.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate table years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not sure we will have enough memory for this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating mock datasets to play with\n",
    "df=pd.read_csv(\"./sample_HES/HES_Sample_APC_neg.txt\", sep=\"\\t\", low_memory=False)\n",
    "sub1=df.sample(frac=0.3,random_state=200)\n",
    "df1=df.drop(sub1.index)\n",
    "sub2=df1.sample(frac=0.5,random_state=200)\n",
    "sub3=df1.drop(sub2.index)\n",
    "sub1.to_csv(\"./sample_HES/HES_Sample_APC_neg_1.txt\", sep=\"\\t\",index=False)\n",
    "sub2.to_csv(\"./sample_HES/HES_Sample_APC_neg_2.txt\", sep=\"\\t\",index=False)\n",
    "sub3.to_csv(\"./sample_HES/HES_Sample_APC_neg_3.txt\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./sample_HES\\\\HES_Sample_APC_neg_1.txt', './sample_HES\\\\HES_Sample_APC_neg_2.txt', './sample_HES\\\\HES_Sample_APC_neg_3.txt']\n",
      "['./sample_HES/HES_Sample_APC_neg_1.txt', './sample_HES/HES_Sample_APC_neg_2.txt', './sample_HES/HES_Sample_APC_neg_3.txt']\n"
     ]
    }
   ],
   "source": [
    "#listing files with same root (hopefully different years will have similar names)\n",
    "#on Linux no need to change backlash in forwardlash\n",
    "subfiles_name=glob.glob(\"./sample_HES/HES_Sample_APC_neg_*.txt\")\n",
    "print(subfiles_name)\n",
    "subfiles_name_OS=[]\n",
    "for name in subfiles_name:\n",
    "    subfiles_name_OS.append(name.replace('\\\\', '/'))\n",
    "print(subfiles_name_OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that given a list of files ALL WITH THE SAME FORMAT, produce a concatenated file of the same format\n",
    "def concatenate_tables(list_tables, fileout, sep=\"\\t\"):\n",
    "    \"\"\"\n",
    "    --Parameters--\n",
    "    list_tables: list of strings \n",
    "              List of files to be concatenated\n",
    "    fileout: str \n",
    "             Name of the concatenated file in output\n",
    "    sep: str \n",
    "         Optional. File separator for read_csv.\n",
    "    --Returns--\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(list_tables[0], sep=sep, low_memory=False)\n",
    "    for chunk in list_tables[1:]:\n",
    "        new_df=pd.read_csv(chunk, sep=\"\\t\", low_memory=False)\n",
    "        df=pd.concat([df, new_df])\n",
    "     #write the concatenated table\n",
    "    print('Writing {:s}'.format(fileout))\n",
    "    df.to_csv(fileout, index=False, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./sample_HES/HES_Sample_APC_neg_all.txt\n"
     ]
    }
   ],
   "source": [
    "concatenate_tables(subfiles_name_OS, sep=\"\\t\", fileout='./sample_HES/HES_Sample_APC_neg_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alternative we can paste directly on a file, if everything fit in memory (they are separate tables), it is worthy to do more operations while in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk by Patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that given a csv file, chunk it in N csv files such that each patient history is contained in one chunk\n",
    "#For each csv file another txt file containing the list of patient IDs contained in that chunk is produced\n",
    "def chunk_by_patient(filename, N_chunk, ID_label, path_ID_files='./') :\n",
    "    \"\"\"\n",
    "    --Parameters--\n",
    "    filename: str \n",
    "             File path of the table to be chunked\n",
    "    N_chunk: integer\n",
    "             Number of desired chunks\n",
    "    ID_label: str \n",
    "             Name of the column that contains the patient IDS (different for each type of dataset)\n",
    "    path_ID_files: str\n",
    "             Optional. Output folder for IDs lists. Default is the current path.\n",
    "    --Returns--\n",
    "    \"\"\"          \n",
    "    df=pd.read_csv(filename,  low_memory=False)\n",
    "    #dividing by number of patients might produce files of quite different size, but it is the easiest solution\n",
    "    IDs_by_chunk= round(len(df[ID_label].unique())/N_chunk)\n",
    "    ID_initial=0\n",
    "    for i in range(0, N_chunk):\n",
    "        #print(i)\n",
    "        if i != N_chunk-1:\n",
    "            ID_chunk=(i+1)*IDs_by_chunk\n",
    "            #IDs are sorted before chunking. This might be convenient for following analysis but not necessary\n",
    "            chunk=df[df[ID_label].isin(df[ID_label].sort_values().unique()[ID_initial:ID_chunk])]\n",
    "            ID_initial= ID_chunk\n",
    "        else:\n",
    "            #since we are rounding last chunk might have less ID than IDs_by_chunk\n",
    "            chunk=df[df[ID_label].isin(df[ID_label].sort_values().unique()[ID_initial:])]\n",
    "        suffix='_chunk'+str(i)\n",
    "        #The chunks will have the same root of the initial file and will be placed in the same folder\n",
    "        fileout=outfile_name(filename, suffix=suffix)\n",
    "        root_IDfile=ID_label\n",
    "        #By default the list od IDS are outputed in the current path folder, this can be regulated by path_ID_files\n",
    "        fileoutID=path_ID_files + root_IDfile + suffix +'.txt'\n",
    "        #write the nth chunk\n",
    "        print('Writing {:s}'.format(fileout))\n",
    "        chunk.to_csv(fileout, index=False)\n",
    "        #write the list of IDs contained in the nth chunk\n",
    "        print('Writing {:s}'.format(fileoutID))\n",
    "        chunk[ID_label].to_csv(fileoutID, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./sample_HES/HES_Sample_APC_neg_all_chunk0.csv\n",
      "Writing ./sample_HES/ENCRYPTED_HESID_chunk0.txt\n",
      "Writing ./sample_HES/HES_Sample_APC_neg_all_chunk1.csv\n",
      "Writing ./sample_HES/ENCRYPTED_HESID_chunk1.txt\n",
      "Writing ./sample_HES/HES_Sample_APC_neg_all_chunk2.csv\n",
      "Writing ./sample_HES/ENCRYPTED_HESID_chunk2.txt\n"
     ]
    }
   ],
   "source": [
    "filename='./sample_HES/HES_Sample_APC_neg_all.csv'\n",
    "N_chunk=3\n",
    "ID_label='ENCRYPTED_HESID'\n",
    "path_ID_files='./sample_HES/'\n",
    "chunk_by_patient(filename, N_chunk, ID_label,path_ID_files=path_ID_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308,)\n",
      "(308,)\n",
      "(307,)\n"
     ]
    }
   ],
   "source": [
    "#Test IDs contained in chunk 0 must not be contained in chunk 1 and so on\n",
    "chunk0=pd.read_csv('./sample_HES/HES_Sample_APC_neg_all_chunk0.csv', low_memory=False)\n",
    "chunk1=pd.read_csv('./sample_HES/HES_Sample_APC_neg_all_chunk1.csv', low_memory=False)\n",
    "chunk2=pd.read_csv('./sample_HES/HES_Sample_APC_neg_all_chunk2.csv', low_memory=False)\n",
    "IDs_chunk0=chunk0[ID_label].unique()\n",
    "IDs_chunk1=chunk1[ID_label].unique()\n",
    "IDs_chunk2=chunk2[ID_label].unique()\n",
    "print(IDs_chunk0.shape)\n",
    "print(IDs_chunk1.shape)\n",
    "print(IDs_chunk2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(~chunk2[ID_label].isin(chunk1[ID_label]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
